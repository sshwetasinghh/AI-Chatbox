{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c92decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Create folders\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "Path(\"chunks\").mkdir(exist_ok=True)\n",
    "Path(\"vectordb\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bde942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Chunks and index saved.\n"
     ]
    }
   ],
   "source": [
    "import os, json, faiss\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load PDF\n",
    "pdf = PdfReader(\"data/AI Training Document.pdf\")\n",
    "text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "# Chunk\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_text(text)\n",
    "with open(\"chunks/chunks.json\", \"w\") as f:\n",
    "    json.dump(chunks, f)\n",
    "\n",
    "# Embed\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "# Save to FAISS\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"vectordb/index.faiss\")\n",
    "with open(\"vectordb/chunk_map.json\", \"w\") as f:\n",
    "    json.dump(chunks, f)\n",
    "\n",
    "print(\"Preprocessing complete. Chunks and index saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12fb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, index_path, chunk_path):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        with open(chunk_path, \"r\") as f:\n",
    "            self.chunks = json.load(f)\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def retrieve(self, query, k=3):\n",
    "        emb = self.model.encode([query])\n",
    "        D, I = self.index.search(emb, k)\n",
    "        return [self.chunks[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self):\n",
    "        genai.configure(api_key=\"AIzaSyCrIUhbEH46MFcyI5h2IW-4psnChwvWf6Y\")\n",
    "        self.model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    def generate(self, context, question):\n",
    "        prompt = f\"Use the context below to answer:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "202448ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        self.retriever = Retriever(\"vectordb/index.faiss\", \"vectordb/chunk_map.json\")\n",
    "        self.generator = Generator()\n",
    "\n",
    "    def run(self, query):\n",
    "        chunks = self.retriever.retrieve(query)\n",
    "        context = \"\\n\\n\".join(chunks)\n",
    "        answer = self.generator.generate(context, query)\n",
    "        return answer, chunks\n",
    "\n",
    "# Initialize\n",
    "rag = RAGPipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5f86cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " The provided text is a snippet of a legal document, likely a User Agreement.  Its purpose is to define the user's responsibilities regarding the content they provide, ensuring they have the necessary rights and that the content is accurate, appropriate, and legal.  It also mentions arbitration as a potential dispute resolution method.\n",
      "\n",
      "Source Chunks:\n",
      "\n",
      "- an improper purpose (as measured by the standards set forth in Federal Rule of Civil\n",
      "Procedure 11(b)). To the extent, following a presentation on the merits, on it s own motion or\n",
      "a partyâ€™s, and after affording a reasonable opportunity to respond, an arbitrator determines\n",
      "- connection with our, those assignees', and those subl icensees' use of that content in connection with\n",
      "our provision, expansion, and promotion of our Services.\n",
      "You represent and warrant that, for all such content you provide, you own or otherwise control all\n",
      "- necessary rights to do so and to meet your obligations under this User Agreement. You represent and\n",
      "warrant that such content is accurate, appropriate, and l egal. You represent and warrant that use of\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the purpose of this Document?\"\n",
    "answer, source_chunks = rag.run(query)\n",
    "\n",
    "print(\"Answer:\\n\", answer)\n",
    "print(\"\\nSource Chunks:\\n\")\n",
    "for chunk in source_chunks:\n",
    "    print(\"-\", chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f84024",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rag_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrag_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPipeline\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set up the page\u001b[39;00m\n\u001b[32m      5\u001b[39m st.set_page_config(page_title=\u001b[33m\"\u001b[39m\u001b[33mðŸ“„ RAG Chatbot (Gemini)\u001b[39m\u001b[33m\"\u001b[39m, layout=\u001b[33m\"\u001b[39m\u001b[33mwide\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rag_pipeline'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from rag_pipeline import RAGPipeline\n",
    "\n",
    "# Set up the page\n",
    "st.set_page_config(page_title=\"ðŸ“„ RAG Chatbot (Gemini)\", layout=\"wide\")\n",
    "st.title(\"ðŸ¤– Ask Questions About the PDF (Gemini RAG)\")\n",
    "\n",
    "# Initialize RAG\n",
    "rag = RAGPipeline()\n",
    "\n",
    "# Chat history in session state\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# Input box\n",
    "query = st.text_input(\"ðŸ’¬ Ask a question about the document:\", key=\"query_input\")\n",
    "\n",
    "# Handle question\n",
    "if query:\n",
    "    with st.spinner(\"ðŸ”Ž Searching... Generating answer...\"):\n",
    "        answer, sources = rag.run(query)\n",
    "        st.session_state.chat_history.append({\n",
    "            \"question\": query,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        })\n",
    "\n",
    "# Show chat history\n",
    "for chat in reversed(st.session_state.chat_history):\n",
    "    st.markdown(f\"**ðŸ§‘ You:** {chat['question']}\")\n",
    "    st.markdown(f\"**ðŸ¤– Gemini:** {chat['answer']}\")\n",
    "    with st.expander(\"ðŸ“š Source Chunks\"):\n",
    "        for chunk in chat['sources']:\n",
    "            st.markdown(f\"- {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a4994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
